from typing import Type

from sgr_deep_research.core.agents.base_agent import BaseAgent
from sgr_deep_research.core.tools import (
    BaseTool,
    CreateReportTool,
    FinalAnswerTool,
    NextStepToolsBuilder,
    NextStepToolStub,
    ReasoningTool,
    WebSearchTool,
    research_agent_tools,
    system_agent_tools,
)
from sgr_deep_research.services import MCP2ToolConverter
from sgr_deep_research.settings import get_config

config = get_config()


class SGRResearchAgent(BaseAgent):
    """Agent for deep research tasks using SGR framework."""

    name: str = "sgr_agent"

    def __init__(
        self,
        task: str,
        toolkit: list[Type[BaseTool]] | None = None,
        max_clarifications: int = 3,
        max_iterations: int = 10,
        max_searches: int = 4,
        tracking_token: str | None = None,
        working_directory: str = ".",
        llm_model: str | None = None,
        llm_base_url: str | None = None,
        llm_api_key: str | None = None,
    ):
        super().__init__(
            task=task,
            toolkit=toolkit,
            max_clarifications=max_clarifications,
            max_iterations=max_iterations,
            tracking_token=tracking_token,
            working_directory=working_directory,
            llm_model=llm_model,
            llm_base_url=llm_base_url,
            llm_api_key=llm_api_key,
        )

        self.toolkit = [
            *system_agent_tools,
            *research_agent_tools,
            *MCP2ToolConverter().toolkit,
            *(toolkit or []),
        ]
        self.toolkit.remove(ReasoningTool)  # we use our own reasoning scheme
        self.max_searches = max_searches

    async def _prepare_tools(self) -> Type[NextStepToolStub]:
        """Prepare tool classes with current context limits."""
        tools = set(self.toolkit)
        if self._context.iteration >= self.max_iterations:
            tools = {
                CreateReportTool,
                FinalAnswerTool,
            }
        if self._context.searches_used >= self.max_searches:
            tools -= {
                WebSearchTool,
            }
        return NextStepToolsBuilder.build_NextStepTools(list(tools))

    async def _reasoning_phase(self) -> NextStepToolStub:
        request_kwargs = {
            "model": self.llm_model,
            "response_format": await self._prepare_tools(),
            "messages": await self._prepare_context(),
            "max_tokens": config.openai.max_tokens,
            "temperature": config.openai.temperature,
            "extra_body": self._get_extra_body(),
        }
        
        # Add stream_options only for providers that support it
        stream_options = self._get_stream_options()
        if stream_options:
            request_kwargs["stream_options"] = stream_options
        
        last_chunk_usage = None  # Capture usage from final chunk (for Cerebras)
        async with self.openai_client.chat.completions.stream(**request_kwargs) as stream:
            async for event in stream:
                if event.type == "chunk":
                    self.streaming_generator.add_chunk(event)
                    # Cerebras returns usage in final chunk - check multiple ways
                    chunk = event.chunk if hasattr(event, 'chunk') else event
                    if hasattr(chunk, 'usage') and chunk.usage:
                        last_chunk_usage = chunk.usage
                    elif hasattr(chunk, 'model_extra') and chunk.model_extra.get('usage'):
                        last_chunk_usage = chunk.model_extra['usage']
        completion = await stream.get_final_completion()
        # Track token usage - prefer completion.usage, fallback to last chunk
        if completion.usage:
            self.token_usage.add_usage(completion.usage)
        elif last_chunk_usage:
            self.token_usage.add_usage(last_chunk_usage)
        reasoning: NextStepToolStub = completion.choices[0].message.parsed  # type: ignore
        # we are not fully sure if it should be in conversation or not. Looks like not necessary data
        # self.conversation.append({"role": "assistant", "content": reasoning.model_dump_json(exclude={"function"})})
        self._log_reasoning(reasoning)
        return reasoning

    async def _select_action_phase(self, reasoning: NextStepToolStub) -> BaseTool:
        tool = reasoning.function
        if not isinstance(tool, BaseTool):
            raise ValueError("Selected tool is not a valid BaseTool instance")
        self.conversation.append(
            {
                "role": "assistant",
                "content": reasoning.remaining_steps[0] if reasoning.remaining_steps else "Completing",
                "tool_calls": [
                    {
                        "type": "function",
                        "id": f"{self._context.iteration}-action",
                        "function": {
                            "name": tool.tool_name,
                            "arguments": tool.model_dump_json(),
                        },
                    }
                ],
            }
        )
        self.streaming_generator.add_tool_call(
            f"{self._context.iteration}-action", tool.tool_name, tool.model_dump_json()
        )
        return tool

    async def _action_phase(self, tool: BaseTool) -> str:
        result = await tool(self._context)
        self.conversation.append(
            {"role": "tool", "content": result, "tool_call_id": f"{self._context.iteration}-action"}
        )
        self.streaming_generator.add_chunk_from_str(f"{result}\n")
        self._log_tool_execution(tool, result)
        return result


if __name__ == "__main__":
    import asyncio

    async def main():
        await MCP2ToolConverter().build_tools_from_mcp()
        agent = SGRResearchAgent(
            task="найди информацию о репозитории на гитхаб sgr-deep-research и ответь на вопрос, "
            "какая основная концепция этого репозитория?",
            max_iterations=5,
            max_clarifications=2,
            max_searches=3,
        )
        await agent.execute()

    asyncio.run(main())
